# -*- coding: utf-8 -*-
"""Week_5_Web_Scrapping(SAI VARMA).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e_p24ToAbmijVVOp29yWOlEIeNDuh0-t

- Importing Libraries
"""

import requests
from bs4 import BeautifulSoup as bs
import pandas as pd

"""+ Url of the website we want to scrape : http://books.toscrape.com/"""

url = 'http://books.toscrape.com/'

"""- Use the requests.get() function to send an HTTP GET request to the specified URL and store the response."""

response = requests.get(url)
response

"""- The line response.status_code is used to check the HTTP status code returned by the web server in response to the HTTP request"""

response.status_code

type(response.text) #type of the Object Response

"""- The line print(response.text[:100]) is printing the first 100 characters of the text content received in the HTTP response."""

response.text[:100]

"""- bs(response.text): This creates a BeautifulSoup object (soup) by parsing the HTML content of the response.text. response.text is the text content of the HTTP response received from the server."""

soup = bs(response.text)

type(soup)

"""- Below line is using BeautifulSoup to find title tag within the BeautifulSoup object (soup)



"""

soup.title.text.strip() #title tag

"""- The line below is using BeautifulSoup to find all HTML elements with the tag 'a' and the class attribute set to 'product_pod' within the BeautifulSoup object (soup)"""

books_tag = soup.find_all('article',class_ ='product_pod')

"""- This line gives no of elements in the books tag"""

len(books_tag)

"""- Used to get 1st element of books tag"""

book = books_tag[0]
book

"""- Extracting the 'title' of a book from the HTML structure of the selected book"""

title_tag = book.find('a',title=True)['title']
title_tag

"""- Extracting the 'Rating' of a book from the HTML structure of the selected book"""

rating_tag = book.find('p')['class'][1]
rating_tag

"""- Extracting the 'price' of a book from the HTML structure of the selected book"""

price_tag = book.find('p',class_ = 'price_color').text[1:]
price_tag

"""- Extracting the 'link' of a book from the HTML structure of the selected book"""

link_tag = book.find('a')['href']
url + link_tag

list_data =[] # Creating a list to store the data after being scraped

"""- The code is part of a loop that iterates over the first 20 books in the 'books_tag' list, extracts information for each book, and appends it to the 'list_data'."""

for book in books_tag:
  title_tag = book.find('a',title=True)['title']
  rating_tag = book.find('p')['class'][1]
  price_tag = book.find('p',class_ = 'price_color').text[1:]
  link_tag = url + book.find('a')['href']

  list_data.append([title_tag,rating_tag,price_tag,link_tag])

columns = ['title','rating','price','link']

"""- This creates a DataFrame from list using pandas"""

data = pd.DataFrame(list_data,columns=columns)

"""- Printing complete data of One page"""

data # complete data of one page

"""#checking for url"""

for i in range(1,5):
  url1 = url + str('/catalogue/page-')+ str(i) +str('.html')
  print(url1)

"""## **Steps to follow to scrape data from all pages**
- Loop through pages
- Construct URL for each page
- Make an HTTP request and create BeautifulSoup object
- Find all book articles on the page
- Extract data for each book on the page
- Append the data to the list
"""

data_list =[]
total_pages = 50
for page in range(1,total_pages+1):                              # loop through pages
  url1 = url + str('/catalogue/page-')+ str(i) +str('.html')     # URL construction
  response = requests.get(url1)                                  # Make HTTP request
  soup = bs(response.text)
  books_tag = soup.find_all('article',class_ = 'product_pod')    # Find all book articles on the page
  for book in books_tag:                                         # Data for each book on the page is extracted
    title_tag = book.find('a',title=True)['title']
    rating_tag = book.find('p')['class'][1]
    price_tag = book.find('p',class_ = 'price_color').text[1:]
    link_tag = url + book.find('a')['href']
    data_list.append([title_tag,rating_tag,price_tag,link_tag])  # Appending the data to list

"""#List of data that contains complete information of all pages"""

data_list[:5]

"""#Converting a list of pages data into a DataFrame using pandas"""

complete_df = pd.DataFrame(data_list, columns = columns)

complete_df

complete_df.shape

"""- Converting the DataFrame to .csv file for further use"""

complete_df.to_csv('books_data.csv', index=False)

"""#Using try except"""

import numpy as np
import time

dl =[]
total_pages = 50
for page in range(1,total_pages+1):                              # loop through pages
  url1 = url + str('/catalogue/page-')+ str(i) +str('.html')     # URL construction
  try:
    response = requests.get(url1)                                  # Make HTTP request
  except Exception as e:
    print(e)
  if (response.status_code == 200):
    soup = bs(response.text)
    try:
      books_tag = soup.find_all('article',class_ = 'product_pod')    # Find all book articles on the page
      for book in books_tag:
        try:
          title_tag = book.find('a',title=True)['title']
        except:
          title_tag = np.nan

        try:
          rating_tag = book.find('p')['class'][1]
        except:
          rating_tag = np.nan

        try:
          price_tag = book.find('p',class_ = 'price_color').text[1:]
        except:
          price_tag = np.nan

        try:
          link_tag = url + book.find('a')['href']
        except:
          link_tag = np.nan

        dl.append([title_tag,rating_tag,price_tag,link_tag])  # Appending the data to list
    except:
      print(f'Error reading page {page}')
    time.sleep(5)

dl[:3]

complete_data = pd.DataFrame(data_list, columns = columns)

complete_data.head()

complete_data.to_csv('data_of_books.csv', index=False)